{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script to transform a json file which is composed by a list of dictionaries, into a dictionary of dictionaries. The first \"value\" (with key = \"collecting institution\") of the previous dictionaries is set as the key for each new dictionary, and the values are the rest of the key/value pairs. Also included new keys and values based on previous data (which is very dependant to this precise requeriment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/pmata/git_repositories/laboratory_address.json\") as file:\n",
    "    orilist = json.load(file)\n",
    "    final_dict={}\n",
    "    for x in orilist:\n",
    "        first_val = list(x.values())[0]\n",
    "        x[\"submitting_institution\"] = x[\"collecting_institution\"]\n",
    "        x[\"submitting_institution_address\"] = x[\"collecting_institution_address\"]\n",
    "        x[\"submitting_institution_email\"] = x[\"collecting_institution_email\"]\n",
    "        x.pop(\"collecting_institution\")\n",
    "        final_dict[first_val] = x\n",
    "print(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/pmata/git_repositories/relecov-tools/relecov_tools/conf/laboratory_address2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if any of the records in the result file after using relecov-tools read-lab-metadata.py has no missing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n"
     ]
    }
   ],
   "source": [
    "with open(\"/tmp/relecov/processed_metadata_lab_COD-2129-AND-FIBAO_20230221.json\") as file:\n",
    "    newlist = json.load(file)\n",
    "    original = len(newlist[0].keys())\n",
    "    for x in newlist:\n",
    "        if len(x.keys()) != original:\n",
    "            print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import relecov_tools.utils\n",
    "from relecov_tools.config_json import ConfigJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "config_json = ConfigJson()\n",
    "sftp_port = config_json.get_topic_data(\"sftp_handle\", \"sftp_port\")\n",
    "print(sftp_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_R1_', '_R1.', '.R1.', 'Forward', 'forward', '_1_', '_1.', '.1.']\n"
     ]
    }
   ],
   "source": [
    "metadata_tmp_folder = config_json.get_topic_data(\n",
    "            \"sftp_handle\", \"allowed_R1_delimiters\"\n",
    "                    )\n",
    "print(metadata_tmp_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import relecov_tools.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_dicter(schema):\n",
    "    schema_json = relecov_tools.utils.read_json_file(f\"/home/pmata/git_repositories/relecov-tools/relecov_tools/schema/{schema}\")\n",
    "    schema_dict = {}\n",
    "    for key,value in schema_json[\"properties\"].items():\n",
    "        schema_dict[key] = {x:y for x,y in value.items() if x == \"ontology\" or x == \"label\"}\n",
    "    label_dict = {}\n",
    "    for x in schema_dict.values():\n",
    "        for key,value in x.items():\n",
    "            if key == \"label\":\n",
    "                label_dict[value] = x[\"ontology\"]\n",
    "    return label_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phagedict = schema_dicter(\"phage_schema.json\")\n",
    "enadict = schema_dicter(\"ena_schema.json\")\n",
    "reledict = schema_dicter(\"relecov_schema.json\")\n",
    "gisaidict = schema_dicter(\"gisaid_schema.json\")\n",
    "dictlist = {\"phage\": phagedict, \"ena\": enadict ,\"relecov\": reledict, \"gisaid\":gisaidict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_differ(schema1, schema2):\n",
    "    same_ontologies = {k: schema1[k] for k in schema1 if k in schema2 and schema1[k] == schema2[k]}\n",
    "    diff_ontologies = {k: schema1[k] for k in schema1 if k in schema2 and schema1[k] != schema2[k]}\n",
    "    \n",
    "    return same_ontologies, diff_ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 {}\n",
      "14 {'Sample ID given by originating laboratory': 'GENEPIO:0001123', 'Sample Received Date': 'NCIT:C93644', 'Organism': 'NCIT:C43459'}\n",
      "9 {'Additional Host Information': 'NCIT:C83280'}\n",
      "14 {}\n",
      "67 {'Sample ID given by originating laboratory': 'GENEPIO:0001123', 'Sample Received Date': 'NCIT:C93644', 'Organism': 'NCIT:C43459', 'Consensus sequence name': 'GENEPIO:0001461'}\n",
      "14 {'Sample ID given by originating laboratory': 'GENEPIO:0001123'}\n"
     ]
    }
   ],
   "source": [
    "print(len(schema_differ(enadict, reledict)[0]), schema_differ(enadict, reledict)[1])\n",
    "print(len(schema_differ(enadict, phagedict)[0]), schema_differ(enadict, phagedict)[1])\n",
    "print(len(schema_differ(enadict, gisaidict)[0]), schema_differ(enadict, gisaidict)[1])\n",
    "print(len(schema_differ(reledict, gisaidict)[0]), schema_differ(reledict, gisaidict)[1])\n",
    "print(len(schema_differ(reledict, phagedict)[0]), schema_differ(reledict, phagedict)[1])\n",
    "print(len(schema_differ(gisaidict, phagedict)[0]), schema_differ(gisaidict, phagedict)[1])\n",
    "\n",
    "#Debugging in-place with import pdb; pdb.set_trace()\n",
    "\n",
    "# Enviar los errores por correo\n",
    "# logging.handlers.SMTPHandler(mailhost=(\"smtp.gmail.com\", 465), fromaddr=correo_isciii, toaddrs=correo_usuario, subject=\"Validation errors\", credentials=(usurario,contraseÃ±a), secure=None, timeout=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "samples_ref = pd.read_csv(\"/data/bi/research/20230323_TFM_PABLO_MA_RELECOV/ANALYSIS/20230324_ANALYSIS01_AMPLICONS_HUMAN/samples_ref.txt\", sep = \"\\t\", header=None)\n",
    "string_head = \"run\\tuser\\thost\\tVirussequence\\tsample\\ttotalreads\\treadshostR1\\treadshost\\t%readshost\\treadsvirus\\t%readsvirus\\tunmappedreads\\t%unmapedreads\\tmedianDPcoveragevirus\\tCoverage>10x(%)\\tVariantsinconsensusx10\\tMissenseVariants\\t%Ns10x\"\n",
    "print(len(string_head.split(\"\\t\")))\n",
    "samples_list = list(samples_ref[0])\n",
    "reference_list = list(samples_ref[1])\n",
    "sample_ref_dict = dict(zip(samples_list, reference_list))\n",
    "sample_ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_enumer(schema1, schema2):\n",
    "    schema_json1 = relecov_tools.utils.read_json_file(f\"/home/pmata/git_repositories/relecov-tools/relecov_tools/schema/{schema1}\")\n",
    "    schema_json2 = relecov_tools.utils.read_json_file(f\"/home/pmata/git_repositories/relecov-tools/relecov_tools/schema/{schema2}\")\n",
    "    schema_dict1 = {}\n",
    "    for key,value in schema_json1[\"properties\"].items():\n",
    "        if \"Enums\" in value.keys():\n",
    "            schema_dict1[key] = [y for x,y in value.items() if x == \"Enums\"]\n",
    "    schema_dict2 = {}\n",
    "    for key,value in schema_json2[\"properties\"].items():\n",
    "        if \"Enums\" in value.keys():\n",
    "            schema_dict2[key] = [y for x,y in value.items() if x == \"Enums\"]\n",
    "\n",
    "    enums1 = [x[:-1].split(\" [\") for k,y in schema_dict1.items() for x in y[0]]\n",
    "    enums2 = [x[:-1].split(\" [\") for k,y in schema_dict2.items() for x in y[0]]\n",
    "\n",
    "    same_enums = {x[0]: [x[1],y[1]] for x in enums1 for y in enums2 if x[0] == y[0] and x[1] == y[1]}\n",
    "    diff_enums = {x[0]: [x[1],y[1]] for x in enums1 for y in enums2 if x[0] == y[0] and x[1] != y[1]}\n",
    "\n",
    "    return same_enums, diff_enums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "phagedict2 = \"phage_schema.json\"\n",
    "enadict2 = \"ena_schema.json\"\n",
    "reledict2 = \"relecov_schema.json\"\n",
    "gisaidict2 = \"gisaid_schema.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdict = {'Research': ['GENEPIO:0100022', 'dfsdf [GENEPIO:0100003'], 'Skin': ['UBERON:0001052', 'sdfsdfd [UBERON:0001003'], 'Middle Nasal Turbinate': ['UBERON:0001762', 'UBERON:0005921'], 'Air filter': ['GENEPIO:0100031', 'ENVO:00003968'], 'Blood Collection Tube': ['GENEPIO:0100031', 'OBI:0002859'], 'Bronchoscope': ['NCIT:C17611', 'OBI:0002826'], 'Collection Container': ['NCIT:C43446', 'OBI:0002088'], 'Filter': ['NCIT:C45801', 'GENEPIO:0100103'], 'Needle': ['NCIT:C69013', 'OBI:0000436'], 'Serum Collection Tube': ['NCIT:C113675', 'OBI:0002860'], 'Sputum Collection Tube': ['GENEPIO:0002115', 'OBI:0002861'], 'Suction Catheter': ['58253008', 'OBI:0002831'], 'Needle Biopsy': ['OBI:0002654', 'OBI:0002651'], 'Filtration': ['NCIT:C16583', 'OBI:0302885'], 'Lavage': ['NCIT:C38068', 'OBI:0600044'], 'Necropsy': ['NCIT:C166270', 'MMO:0000344'], 'Swabbing': ['NCIT:C17627', 'GENEPIO:0002117'], 'Inferior Nasal Turbinate': ['NCIT:C32794', 'UBERON:0005922'], 'Biopsy': ['NCIT:C15189', 'OBI:0002650'], 'Daycare': ['ENVO:01000927', 'GENEPIO:0100193'], 'Hospital': ['ENVO:00002173', 'ECTO:1000035'], 'School': ['ENVO:03501130', 'GENEPIO:0100224']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Research': 'GENEPIO:0100022 dfsdf',\n",
       " 'Skin': 'UBERON:0001052 sdfsdfd',\n",
       " 'Middle Nasal Turbinate': 'UBERON:0001762 UBERON:0005921',\n",
       " 'Air filter': 'GENEPIO:0100031 ENVO:00003968',\n",
       " 'Blood Collection Tube': 'GENEPIO:0100031 OBI:0002859',\n",
       " 'Bronchoscope': 'NCIT:C17611 OBI:0002826',\n",
       " 'Collection Container': 'NCIT:C43446 OBI:0002088',\n",
       " 'Filter': 'NCIT:C45801 GENEPIO:0100103',\n",
       " 'Needle': 'NCIT:C69013 OBI:0000436',\n",
       " 'Serum Collection Tube': 'NCIT:C113675 OBI:0002860',\n",
       " 'Sputum Collection Tube': 'GENEPIO:0002115 OBI:0002861',\n",
       " 'Suction Catheter': '58253008 OBI:0002831',\n",
       " 'Needle Biopsy': 'OBI:0002654 OBI:0002651',\n",
       " 'Filtration': 'NCIT:C16583 OBI:0302885',\n",
       " 'Lavage': 'NCIT:C38068 OBI:0600044',\n",
       " 'Necropsy': 'NCIT:C166270 MMO:0000344',\n",
       " 'Swabbing': 'NCIT:C17627 GENEPIO:0002117',\n",
       " 'Inferior Nasal Turbinate': 'NCIT:C32794 UBERON:0005922',\n",
       " 'Biopsy': 'NCIT:C15189 OBI:0002650',\n",
       " 'Daycare': 'ENVO:01000927 GENEPIO:0100193',\n",
       " 'Hospital': 'ENVO:00002173 ECTO:1000035',\n",
       " 'School': 'ENVO:03501130 GENEPIO:0100224'}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_to_format = {x: \" \".join([f.split(\" [\", 1)[0] for f in y]) \n",
    "            for x,y in newdict.items()}\n",
    "\n",
    "fields_to_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 {}\n",
      "401 {}\n",
      "20 {}\n",
      "19 {}\n",
      "520 {'Research': ['GENEPIO:0100022', 'GENEPIO:0100003'], 'Skin': ['UBERON:0001052', 'UBERON:0001003'], 'Middle Nasal Turbinate': ['UBERON:0001762', 'UBERON:0005921'], 'Air filter': ['GENEPIO:0100031', 'ENVO:00003968'], 'Blood Collection Tube': ['GENEPIO:0100031', 'OBI:0002859'], 'Bronchoscope': ['NCIT:C17611', 'OBI:0002826'], 'Collection Container': ['NCIT:C43446', 'OBI:0002088'], 'Filter': ['NCIT:C45801', 'GENEPIO:0100103'], 'Needle': ['NCIT:C69013', 'OBI:0000436'], 'Serum Collection Tube': ['NCIT:C113675', 'OBI:0002860'], 'Sputum Collection Tube': ['GENEPIO:0002115', 'OBI:0002861'], 'Suction Catheter': ['58253008', 'OBI:0002831'], 'Needle Biopsy': ['OBI:0002654', 'OBI:0002651'], 'Filtration': ['NCIT:C16583', 'OBI:0302885'], 'Lavage': ['NCIT:C38068', 'OBI:0600044'], 'Necropsy': ['NCIT:C166270', 'MMO:0000344'], 'Swabbing': ['NCIT:C17627', 'GENEPIO:0002117'], 'Inferior Nasal Turbinate': ['NCIT:C32794', 'UBERON:0005922'], 'Biopsy': ['NCIT:C15189', 'OBI:0002650'], 'Daycare': ['ENVO:01000927', 'GENEPIO:0100193'], 'Hospital': ['ENVO:00002173', 'ECTO:1000035'], 'School': ['ENVO:03501130', 'GENEPIO:0100224']}\n",
      "20 {}\n"
     ]
    }
   ],
   "source": [
    "print(len(schema_enumer(enadict2, reledict2)[0]), schema_enumer(enadict2, reledict2)[1])\n",
    "print(len(schema_enumer(enadict2, phagedict2)[0]), schema_enumer(enadict2, phagedict2)[1])\n",
    "print(len(schema_enumer(enadict2, gisaidict2)[0]), schema_enumer(enadict2, gisaidict2)[1])\n",
    "print(len(schema_enumer(reledict2, gisaidict2)[0]), schema_enumer(reledict2, gisaidict2)[1])\n",
    "print(len(schema_enumer(reledict2, phagedict2)[0]), schema_enumer(reledict2, phagedict2)[1])\n",
    "print(len(schema_enumer(gisaidict2, phagedict2)[0]), schema_enumer(gisaidict2, phagedict2)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {}\n"
     ]
    }
   ],
   "source": [
    "print(len(schema_enumer(enadict2, gisaidict2)[0]), schema_enumer(enadict2, gisaidict2)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/opt/nextstrain/datasets/sars-cov-2-v2/20230511/data/merged_bioinfo_processed_metadata.json\") as file:\n",
    "    merg_bioinfo = json.load(file)\n",
    "samplesource={}\n",
    "for x in merg_bioinfo:\n",
    "    samplesource[x[\"isolate_sample_id\"]] = x[\"collecting_institution\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(samplesource.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"\") as file:\n",
    "    jsonprueba = json.load(file)\n",
    "jsonclean = [x for x in jsonprueba if \"lineage_name\" in x.keys()]\n",
    "\n",
    "with open(\"/data/bioinfoshare/UCCT_Relecov/COD-2100-MAD-CNM/20220208/bioinfo_processed_metadata_partial.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(jsonclean, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018147_R1.fastq.gz',\n",
       " '2018187_R1.fastq.gz',\n",
       " '2018192_R1.fastq.gz',\n",
       " '210010_R1.fastq.gz',\n",
       " '210082_R1.fastq.gz',\n",
       " '210092_R1.fastq.gz',\n",
       " '210098_R1.fastq.gz',\n",
       " '210141-B_R1.fastq.gz',\n",
       " '210195_R1.fastq.gz',\n",
       " '210275_R1.fastq.gz',\n",
       " '210359_R1.fastq.gz',\n",
       " '212527_R1.fastq.gz']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[\"sequence_file_R1_fastq\"] for x in jsonclean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotable = pd.read_csv(\"/home/pmata/Downloads/GCF_009858895.2_ASM985889v3_genomic.gff\", sep=\"\\t\", comment=\"#\", header=None)\n",
    "splitcol = annotable[8].str.split(\";\", expand=True)\n",
    "\n",
    "# Drop the original column containing the \";\"-separated strings\n",
    "annotable = annotable.drop(8, axis=1)\n",
    "\n",
    "# Concatenate the original dataframe with the new columns\n",
    "annotable = pd.concat([annotable, splitcol], axis=1)\n",
    "annotable.columns=[\"Seqid\", \"Source\", \"Type\", \"Start\", \"End\", \n",
    "                   \"Score\", \"Strand\", \"Phase\", \"ID\", \"Dbxref\", \n",
    "                   \"collection-date\", \"country\", \"gb-acronym\", \"gbkey\", \n",
    "                   \"genome\", \"isolate\", \"mol_type\", \"nat-host\", \"old-name\"]\n",
    "annotable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r'(\\w+)=([^;]+);?')\n",
    "annotable = pd.read_csv(\"/home/pmata/Downloads/GCF_009858895.2_ASM985889v3_genomic.gff\", sep=\"\\t\", comment=\"#\", header=None)\n",
    "\n",
    "# Extract the attributes column and split it into multiple columns using the regex\n",
    "attributes = annotable[8].str.extractall(regex).unstack().droplevel(0, axis=1)\n",
    "#df.drop(columns=[8], inplace=True)\n",
    "# Merge the attributes columns with the original DataFrame\n",
    "df = pd.concat([annotable, attributes], axis=1)\n",
    "\n",
    "# Remove the original attributes column\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/pmata/git_repositories/relecov-tools/relecov_tools/conf/geo_loc_cities.json\") as file:\n",
    "    geolocs = json.load(file)\n",
    "\n",
    "import pandas as pd\n",
    "geolocdf = pd.DataFrame.from_dict(geolocs, orient=\"index\"). reset_index()\n",
    "division = [\"city\" for x in range(0,len(geolocs))]\n",
    "geolocdf.insert(loc=0, column=\"division\", value=\"division\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocdf.to_csv(\"/home/pmata/lat_long2.tsv\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/bioinfoshare/UCCT_Relecov/COD-2129-AND-FIBAO/20230331/samples_data_COD-2129-AND-FIBAO_20230331.json\") as file:\n",
    "    samplefile = json.load(file)\n",
    "\n",
    "submit_id_for_collect_id = {samplefile[x][\"sequence_file_R1_fastq\"].split(\".\")[0]:y for x,y in samplefile.items()}\n",
    "submit_id_for_collect_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/bioinfoshare/UCCT_Relecov/COD-2129-AND-FIBAO/20230331/fixed_samples_data_COD-2129-AND-FIBAO_20230331.json\",\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(submit_id_for_collect_id, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AND00132'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplefile[\"1318725\"][\"sequence_file_R1_fastq\"].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"/data/bioinfoshare/UCCT_Relecov/COD-2129-AND-FIBAO/20230420/software_versions.yml\", \"r\") as file:\n",
    "    softvers = file.read()\n",
    "dictvers = yaml.safe_load(softvers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanvers = [x for x in dictvers.values()]\n",
    "listvers=[]\n",
    "for dicte in cleanvers:\n",
    "    for key,value in dicte.items():\n",
    "        newdict = {key:value}\n",
    "        if newdict not in listvers:\n",
    "            listvers.append(newdict)\n",
    "    if dicte in listvers:\n",
    "        continue\n",
    "    else:\n",
    "        listvers.append(dicte)\n",
    "listvers2 = [x for x in listvers if len(x) == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "versdf = pd.DataFrame(listvers2)\n",
    "versdf = versdf.melt(var_name=\"Software\", value_name=\"Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "versdf = versdf.dropna().reset_index(drop=True)\n",
    "versdf = versdf.drop(5)\n",
    "\n",
    "versdf.to_csv(\"/home/pmata/Downloads/software_versions.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "tot = 0\n",
    "num = 0\n",
    "with open(\"/opt/nextstrain/datasets/sars-cov-2-v2/20230511/data/sequences.fasta\") as filesamps:\n",
    "    handle = filesamps.read()\n",
    "for header, group in groupby(handle, lambda x:x.startswith('>')):\n",
    "    if not header:\n",
    "        num += 1\n",
    "        tot += sum(map(lambda x: len(x.strip()), group))\n",
    "result = float(tot)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>China</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>China</td>\n",
       "      <td>Wuhan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Bavaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Guam</td>\n",
       "      <td>Guam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>China</td>\n",
       "      <td>Hangzhou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>India</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>India</td>\n",
       "      <td>Ahmedabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>India</td>\n",
       "      <td>Prantij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>India</td>\n",
       "      <td>Gandhinagar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>India</td>\n",
       "      <td>Himatnagar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>India</td>\n",
       "      <td>Dahegam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>India</td>\n",
       "      <td>Mansa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>India</td>\n",
       "      <td>Botad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>India</td>\n",
       "      <td>Una</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>India</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>India</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Iran</td>\n",
       "      <td>Iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Jamaica</td>\n",
       "      <td>Jamaica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Saitama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>South Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Valencia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>Sri Lanka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>USA</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>USA</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>USA</td>\n",
       "      <td>Connecticut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>USA</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>USA</td>\n",
       "      <td>Idaho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>USA</td>\n",
       "      <td>Indiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>USA</td>\n",
       "      <td>Louisiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>USA</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>USA</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>USA</td>\n",
       "      <td>Illinois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>USA</td>\n",
       "      <td>Nebraska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>USA</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>USA</td>\n",
       "      <td>Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>USA</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>USA</td>\n",
       "      <td>North Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>USA</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>USA</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>USA</td>\n",
       "      <td>Wisconsin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>Uruguay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Ho Chi Minh City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>China</td>\n",
       "      <td>Hubei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country          division\n",
       "0      Australia          Victoria\n",
       "91         China           Beijing\n",
       "92         China             China\n",
       "93         China             Wuhan\n",
       "94           USA           Florida\n",
       "95        France            France\n",
       "105      Germany           Bavaria\n",
       "107       Greece            Athens\n",
       "117         Guam              Guam\n",
       "118        China          Hangzhou\n",
       "119        India             Surat\n",
       "120        India         Ahmedabad\n",
       "124        India           Prantij\n",
       "126        India       Gandhinagar\n",
       "128        India        Himatnagar\n",
       "129        India           Dahegam\n",
       "130        India             Mansa\n",
       "131        India             Botad\n",
       "132        India               Una\n",
       "134        India         Hyderabad\n",
       "135        India             India\n",
       "139         Iran              Iran\n",
       "140      Jamaica           Jamaica\n",
       "141        Japan           Saitama\n",
       "142     Malaysia          Malaysia\n",
       "143  South Korea       South Korea\n",
       "144        Spain          Valencia\n",
       "146        Spain             Spain\n",
       "148    Sri Lanka         Sri Lanka\n",
       "149       Taiwan            Taiwan\n",
       "150     Thailand          Thailand\n",
       "154          USA            Alaska\n",
       "156          USA           Arizona\n",
       "157          USA        California\n",
       "186          USA       Connecticut\n",
       "191          USA        Washington\n",
       "193          USA               USA\n",
       "200          USA           Georgia\n",
       "201          USA            Hawaii\n",
       "203          USA             Idaho\n",
       "207          USA           Indiana\n",
       "208          USA         Louisiana\n",
       "211          USA     Massachusetts\n",
       "216          USA          Michigan\n",
       "220          USA          Illinois\n",
       "249          USA          Nebraska\n",
       "250          USA          New York\n",
       "276          USA              Ohio\n",
       "278          USA      Pennsylvania\n",
       "280          USA    South Carolina\n",
       "281          USA    North Carolina\n",
       "288          USA              Utah\n",
       "305          USA          Virginia\n",
       "405          USA         Wisconsin\n",
       "410      Uruguay           Uruguay\n",
       "411      Vietnam  Ho Chi Minh City\n",
       "414        China             Hubei\n",
       "416  Netherlands       Netherlands"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mett = pd.read_csv(\"/opt/nextstrain/datasets/sars-cov-2-v2/20230517/data/example_metadata.tsv\", sep=\"\\t\")\n",
    "mett[[\"country\",\"division\"]].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relecov_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f3ba3e22d1632f5fbac6df5ba3ec9d9d3912a945952ae01fcccd9bb5f4eed18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
